"""
å¢å¼ºåŸŸåç›‘æ§ç³»ç»Ÿ - å¤šAPIæŸ¥è¯¢ + who.isçˆ¬å– + æŒç»­ç›‘æ§
"""
import asyncio
import json
import csv
import os
import re
import socket
import dns.resolver
from datetime import datetime, timezone
from typing import Dict, List, Optional, Tuple
from pathlib import Path

import httpx
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout


class EnhancedDomainMonitor:
    """å¢å¼ºç‰ˆåŸŸåç›‘æ§å™¨ - å¤šæ•°æ®æº + è‡ªåŠ¨fallback"""
    
    # å®˜æ–¹WHOISæœåŠ¡å™¨åœ°å€
    WHOIS_SERVERS = {
        ".com": "whois.verisign-grs.com",
        ".net": "whois.verisign-grs.com",
        ".org": "whois.pir.org",
        ".nl": "whois.domain-registry.nl",
        ".be": "whois.dns.be",
        ".eu": "whois.eu",
        ".uk": "whois.nic.uk",
        ".de": "whois.denic.de",
        ".fr": "whois.nic.fr",
        ".it": "whois.nic.it",
        ".es": "whois.nic.es",
        ".pt": "whois.dns.pt",
        ".biz": "whois.biz",
        ".info": "whois.afilias.net",
        ".us": "whois.nic.us",
        ".ca": "whois.cira.ca",
        ".au": "whois.auda.org.au",
        ".jp": "whois.jprs.jp",
        ".cn": "whois.cnnic.cn",
        ".in": "whois.registry.in",
        ".io": "whois.nic.io",
        ".co": "whois.nic.co",
        ".me": "whois.nic.me",
        ".tv": "whois.nic.tv",
        ".cc": "whois.nic.cc",
        ".ws": "whois.website.ws",
        ".mobi": "whois.dotmobiregistry.net",
        ".pro": "whois.registry.pro",
        ".tel": "whois.nic.tel",
        ".travel": "whois.nic.travel",
        ".xxx": "whois.nic.xxx",
        ".asia": "whois.nic.asia",
        ".lu": "whois.dns.lu",
    }
    
    # ä¸‰ä¸ªRDAP/WHOIS APIç«¯ç‚¹ï¼ˆä¿ç•™ä½œä¸ºå¤‡ç”¨ï¼‰
    RDAP_ENDPOINTS = {
        "primary": {
            ".com": "https://rdap.verisign.com/com/v1/domain/{}",
            ".net": "https://rdap.verisign.com/net/v1/domain/{}",
            ".org": "https://rdap.publicinterestregistry.org/rdap/domain/{}",
            ".nl": "https://rdap.sidn.nl/domain/{}",
            ".be": "https://rdap.dns.be/domain/{}",
            ".eu": "https://rdap.eu/domain/{}",
        },
        "secondary": {
            ".com": "https://rdap.arin.net/registry/domain/{}",
            ".net": "https://rdap.arin.net/registry/domain/{}",
        },
        "bootstrap": "https://rdap.org/domain/{}"  # RDAP Bootstrap
    }
    
    def __init__(self, api_ninjas_key: Optional[str] = None, deepseek_key: Optional[str] = None):
        """åˆå§‹åŒ–ç›‘æ§å™¨"""
        self.api_ninjas_key = api_ninjas_key or os.environ.get("API_NINJAS_KEY")
        self.deepseek_key = deepseek_key or os.environ.get("DEEPSEEK_API_KEY")
        self.timeout = httpx.Timeout(30.0)
        self.results_cache = {}
    
    def get_tld(self, domain: str) -> str:
        """æå–TLD"""
        parts = domain.lower().split('.')
        if len(parts) >= 2:
            return '.' + parts[-1]
        return ''
    
    async def query_official_whois(self, domain: str) -> Dict:
        """ç›´æ¥æŸ¥è¯¢å®˜æ–¹WHOISæœåŠ¡å™¨
        
        Args:
            domain: åŸŸå
            
        Returns:
            åŒ…å«åŸå§‹WHOISæ–‡æœ¬çš„å­—å…¸
        """
        tld = self.get_tld(domain)
        whois_server = self.WHOIS_SERVERS.get(tld, "whois.iana.org")
        
        print(f"      ğŸ“¡ æŸ¥è¯¢å®˜æ–¹WHOIS: {whois_server}")
        
        try:
            # ä½¿ç”¨socketç›´æ¥è¿æ¥WHOISæœåŠ¡å™¨
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(10)
            
            # è¿æ¥WHOISæœåŠ¡å™¨ï¼ˆç«¯å£43ï¼‰
            await asyncio.get_event_loop().run_in_executor(
                None, sock.connect, (whois_server, 43)
            )
            
            # å‘é€æŸ¥è¯¢
            query = f"{domain}\r\n"
            await asyncio.get_event_loop().run_in_executor(
                None, sock.sendall, query.encode('utf-8')
            )
            
            # æ¥æ”¶å“åº”
            response_parts = []
            while True:
                chunk = await asyncio.get_event_loop().run_in_executor(
                    None, sock.recv, 4096
                )
                if not chunk:
                    break
                response_parts.append(chunk.decode('utf-8', errors='ignore'))
            
            sock.close()
            
            whois_text = ''.join(response_parts)
            
            if not whois_text or len(whois_text) < 50:
                print(f"      âœ— WHOISå“åº”ä¸ºç©ºæˆ–å¤ªçŸ­")
                return {'source': 'official_whois', 'success': False, 'data': {}}
            
            print(f"      âœ“ è·å–WHOISæ–‡æœ¬: {len(whois_text)} å­—ç¬¦")
            
            # å¦‚æœæœ‰DeepSeekï¼Œä½¿ç”¨LLMè§£æ
            if self.deepseek_key:
                print(f"      ğŸ¤– ä½¿ç”¨DeepSeekè§£æWHOIS...")
                llm_result = await self.parse_with_deepseek(whois_text, domain)
                
                if llm_result:
                    llm_result['raw_whois'] = whois_text[:1000]  # ä¿å­˜å‰1000å­—ç¬¦
                    llm_result['llm_parsed'] = True
                    return {'source': 'official_whois', 'success': True, 'data': llm_result}
            
            # æ²¡æœ‰LLMï¼Œä½¿ç”¨æ­£åˆ™è§£æ
            parsed = self._parse_raw_whois(whois_text)
            parsed['raw_whois'] = whois_text[:1000]
            parsed['llm_parsed'] = False
            
            has_data = any([
                parsed.get('registrar'),
                parsed.get('creation_date'),
                parsed.get('nameservers')
            ])
            
            return {'source': 'official_whois', 'success': has_data, 'data': parsed}
            
        except socket.timeout:
            print(f"      âœ— WHOISæŸ¥è¯¢è¶…æ—¶")
            return {'source': 'official_whois', 'success': False, 'data': {}}
        except Exception as e:
            print(f"      âœ— WHOISæŸ¥è¯¢å¤±è´¥: {e}")
            return {'source': 'official_whois', 'success': False, 'data': {}}
    
    async def query_rdap_primary(self, domain: str) -> Dict:
        """æŸ¥è¯¢ä¸»RDAPæœåŠ¡å™¨"""
        tld = self.get_tld(domain)
        endpoint = self.RDAP_ENDPOINTS["primary"].get(tld)
        
        if not endpoint:
            return {'source': 'rdap_primary', 'success': False, 'data': {}}
        
        url = endpoint.format(domain)
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(url)
                if response.status_code == 200:
                    data = response.json()
                    parsed = self._parse_rdap_response(data, url)
                    return {'source': 'rdap_primary', 'success': True, 'data': parsed}
        except:
            pass
        
        return {'source': 'rdap_primary', 'success': False, 'data': {}}
    
    async def query_rdap_bootstrap(self, domain: str) -> Dict:
        """æŸ¥è¯¢RDAP BootstrapæœåŠ¡"""
        url = self.RDAP_ENDPOINTS["bootstrap"].format(domain)
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(url)
                if response.status_code == 200:
                    data = response.json()
                    parsed = self._parse_rdap_response(data, url)
                    return {'source': 'rdap_bootstrap', 'success': True, 'data': parsed}
        except:
            pass
        
        return {'source': 'rdap_bootstrap', 'success': False, 'data': {}}
    
    def extract_whois_context(self, full_text: str, anchor_text: str = "Raw WHOIS", 
                             before_tokens: int = 100, after_tokens: int = 1900) -> Optional[str]:
        """æå–Raw WHOISé™„è¿‘çš„æ–‡æœ¬å†…å®¹
        
        Args:
            full_text: å®Œæ•´çš„é¡µé¢æ–‡æœ¬
            anchor_text: é”šç‚¹æ–‡æœ¬
            before_tokens: é”šç‚¹å‰çš„tokenæ•°ï¼ˆçº¦ç­‰äºå•è¯æ•°ï¼‰
            after_tokens: é”šç‚¹åçš„tokenæ•°
            
        Returns:
            æå–çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        # æŸ¥æ‰¾é”šç‚¹ä½ç½®
        patterns = [
            "Raw WHOIS responses from registry and registrar servers",
            "Raw WHOIS",
            "WHOIS Record",
            "Domain Information"
        ]
        
        anchor_pos = -1
        for pattern in patterns:
            pos = full_text.find(pattern)
            if pos != -1:
                anchor_pos = pos
                break
        
        if anchor_pos == -1:
            # æ²¡æ‰¾åˆ°é”šç‚¹ï¼Œå°è¯•æå–æ•´ä¸ªWHOISéƒ¨åˆ†
            return None
        
        # ç®€å•çš„tokenåˆ†å‰²ï¼ˆæŒ‰ç©ºæ ¼å’Œæ¢è¡Œï¼‰
        words = full_text.split()
        
        # è®¡ç®—é”šç‚¹åœ¨wordsä¸­çš„ä½ç½®
        text_before_anchor = full_text[:anchor_pos]
        words_before = text_before_anchor.split()
        anchor_word_pos = len(words_before)
        
        # æå–ä¸Šä¸‹æ–‡
        start_pos = max(0, anchor_word_pos - before_tokens)
        end_pos = min(len(words), anchor_word_pos + after_tokens)
        
        context_words = words[start_pos:end_pos]
        context = ' '.join(context_words)
        
        return context
    
    async def parse_with_deepseek(self, whois_text: str, domain: str, source_url: str = None) -> Dict:
        """ä½¿ç”¨DeepSeek LLMè§£æWHOISæ–‡æœ¬
        
        Args:
            whois_text: WHOISæ–‡æœ¬å†…å®¹
            domain: åŸŸå
            source_url: æ•°æ®æºURL (å¯é€‰)
            
        Returns:
            è§£æåçš„ç»“æ„åŒ–æ•°æ®
        """
        if not self.deepseek_key:
            return {}
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        from datetime import datetime, timezone
        timestamp = datetime.now(timezone.utc).isoformat()
        
        # å¦‚æœæ²¡æœ‰æä¾›source_urlï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not source_url:
            source_url = f"https://who.is/whois/{domain}"
        
        prompt = f"""You are a domain registration information extraction engine.

Goal

From the input below, extract domain registration facts into a STRICT JSON table.

You will receive:

1. A small metadata header:
   DATA_SOURCE: {source_url}
   QUERY_TIMESTAMP: {timestamp}

2. Raw HTML or plain text of a WHOIS / RDAP / registrar web page, including any legal notices, rate-limit warnings, or other noise.

Your tasks:

- Detect all domain records contained in the text. There may be one or multiple domains.
- For each domain, create ONE JSON object with the following fields:

  - domain                 (string)
  - registrant_organization (string or null)
  - registrar              (string or null)
  - registry               (string, the TLD with a leading dot, e.g. ".be", ".com")
  - creation_date          (string or null, preferred format "YYYY-MM-DD" if clearly given; otherwise copy the exact date string as-is)
  - expiry_date            (string or null, same rule as creation_date)
  - nameservers            (array of strings, each a nameserver hostname; empty array if none clearly present)
  - data_sources           (array of strings; must at least contain the value from DATA_SOURCE)
  - timestamp              (string; must exactly copy the value from QUERY_TIMESTAMP)

Output format (VERY IMPORTANT):
- Return ONLY a single JSON object, with this exact top-level structure:

{{
  "domains": [
    {{
      "domain": "example.com",
      "registrant_organization": null,
      "registrar": "Example Registrar Ltd.",
      "registry": ".com",
      "creation_date": "2015-06-24",
      "expiry_date": null,
      "nameservers": [
        "ns1.example.net",
        "ns2.example.net"
      ],
      "data_sources": [
        "https://www.example-registrar.com/whois/example.com"
      ],
      "timestamp": "{timestamp}"
    }}
  ]
}}

Extraction rules (CRITICAL):

1. Do NOT invent, infer, or guess values.
   - If a field is not explicitly present in the input, set it to:
     - null for scalar fields (registrant_organization, registrar, creation_date, expiry_date)
     - [] (empty array) for lists (nameservers, data_sources if DATA_SOURCE is missing for some reason).

2. Domain:
   - Use the exact domain labels as they appear in the record (e.g. "aholddelhaize.be").
   - If the page clearly contains only one domain, still output an array with one JSON object.

3. Registrar:
   - Use the value next to labels such as "Registrar:", "Registrar Name:", "Registrar Name" or similar.
   - Copy the registrar name as shown, without modification.

4. Registrant_organization:
   - Use the organization / company name of the registrant if it is explicitly provided under labels such as
     "Registrant Organization:", "Registrant:", "Holder:", "Domain holder", etc.
   - If only a person name or email is shown and it is not clearly an organization, you may still put the exact text
     into registrant_organization.
   - If the registrant is hidden, redacted, or not shown, set registrant_organization to null.
   - NEVER infer the registrant from brand names, website content, or your own knowledge.

5. Registry:
   - Derive from the domain's top-level domain:
       "aholddelhaize.be" -> ".be"
       "example.com"      -> ".com"
       "foo.org"          -> ".org"
   - Always include the leading dot.

6. Creation_date:
   - Look for labels such as "Creation Date", "Created On", "Registered:", "Registered On", "Domain registered:" etc.
   - If multiple date formats appear for the same field, pick the one most clearly linked to domain creation.
   - If the date is clearly a standard format (e.g. "2015-06-24"), keep it as is.
   - If the date is a long string (e.g. "Wed Jun 24 2015"), you may either:
       (a) normalize to "2015-06-24" if it is unambiguous, OR
       (b) copy the full original string.
   - If no creation date is present, set creation_date to null.

7. Expiry_date:
   - Look for labels such as "Expiry Date", "Expiration Date", "Registry Expiry Date", "Renewal date", etc.
   - Apply the same formatting rules as for creation_date.
   - If no expiry date is present, set expiry_date to null.

8. Nameservers:
   - Collect all hostnames under labels such as "Name Server", "Nameservers", "Name servers", etc.
   - Normalize by trimming spaces; keep them as plain strings (no need to lower-case, but you may do so).
   - If no nameservers are clearly listed, use an empty array.

9. Data_sources:
   - Always include the exact string from DATA_SOURCE as one element of the array.
   - If the input text itself clearly lists additional sources (for example: "Data from registry X and registrar Y"), you may add those as extra array elements, but only if they are explicitly named.
   - NEVER fabricate additional sources.

10. Timestamp:
   - Copy the value from QUERY_TIMESTAMP exactly, without modification or reformatting.
   - Do NOT generate your own timestamps.

11. Ignore noise:
   - Completely ignore WHOIS legal disclaimers, terms of use, anti-spam policies, and rate-limit messages.
   - Do NOT place disclaimer text into any field.

12. If the input contains zero recognizable domains:
   - Return {{"domains": []}}

The raw input starts after the line:
=====BEGIN INPUT=====
and ends before the line:
=====END INPUT=====

Now read the input and return ONLY the JSON described above.

=====BEGIN INPUT=====
{whois_text}
=====END INPUT====="""

        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(60.0)) as client:
                response = await client.post(
                    "https://api.deepseek.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.deepseek_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "deepseek-chat",
                        "messages": [
                            {
                                "role": "user",
                                "content": prompt
                            }
                        ],
                        "temperature": 0.1,
                        "max_tokens": 1000
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
                    
                    # æå–JSON
                    # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡
                    import json as json_module
                    
                    # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
                    content = content.replace('```json', '').replace('```', '').strip()
                    
                    try:
                        parsed = json_module.loads(content)
                        
                        # æ–°çš„promptè¿”å› {"domains": [...]} ç»“æ„
                        # æå–ç¬¬ä¸€ä¸ªdomainå¹¶è½¬æ¢ä¸ºæ—§æ ¼å¼ä»¥å…¼å®¹ç°æœ‰ä»£ç 
                        if 'domains' in parsed and len(parsed['domains']) > 0:
                            domain_data = parsed['domains'][0]
                            
                            # æ˜ å°„å­—æ®µåç§° (æ–°æ ¼å¼ -> æ—§æ ¼å¼)
                            result = {
                                'registrar': domain_data.get('registrar'),
                                'registry': domain_data.get('registry'),
                                'registrant_org': domain_data.get('registrant_organization'),  # æ˜ å°„åˆ°æ—§å­—æ®µå
                                'creation_date': domain_data.get('creation_date'),
                                'expiry_date': domain_data.get('expiry_date'),
                                'nameservers': domain_data.get('nameservers', []),
                                'data_source': domain_data.get('data_sources', [source_url])[0] if domain_data.get('data_sources') else source_url,
                                'timestamp': domain_data.get('timestamp'),
                            }
                            
                            print(f"      âœ“ DeepSeekè§£ææˆåŠŸ")
                            return result
                        else:
                            print(f"      âš ï¸ DeepSeekè¿”å›çš„JSONä¸­æ²¡æœ‰domainså­—æ®µæˆ–ä¸ºç©º")
                            return {}
                    except Exception as e:
                        print(f"      âš ï¸ DeepSeekè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSON: {e}")
                        return {}
                else:
                    print(f"      âš ï¸ DeepSeek APIé”™è¯¯: {response.status_code}")
                    return {}
        
        except Exception as e:
            print(f"      âš ï¸ DeepSeekè°ƒç”¨å¤±è´¥: {e}")
            return {}
    
    async def query_api_ninjas(self, domain: str) -> Dict:
        """æŸ¥è¯¢API Ninjas WHOISæœåŠ¡"""
        if not self.api_ninjas_key:
            return {'source': 'api_ninjas', 'success': False, 'data': {}}
        
        url = f"https://api.api-ninjas.com/v1/whois?domain={domain}"
        headers = {"X-Api-Key": self.api_ninjas_key}
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(url, headers=headers)
                if response.status_code == 200:
                    data = response.json()
                    parsed = self._parse_api_ninjas_response(data)
                    return {'source': 'api_ninjas', 'success': True, 'data': parsed}
        except:
            pass
        
        return {'source': 'api_ninjas', 'success': False, 'data': {}}
    
    async def scrape_whois_website(self, domain: str) -> Dict:
        """ä½¿ç”¨Playwrightçˆ¬å–who.isç½‘ç«™ï¼Œå¹¶ç”¨DeepSeek LLMè§£æ"""
        print(f"      ğŸŒ ä½¿ç”¨Playwrightçˆ¬å– who.is: {domain}")
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                )
                page = await context.new_page()
                
                # è®¿é—®who.is
                url = f"https://who.is/whois/{domain}"
                await page.goto(url, wait_until='networkidle', timeout=30000)
                
                # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                await asyncio.sleep(5)
                
                # è·å–æ•´ä¸ªé¡µé¢æ–‡æœ¬
                page_text = await page.inner_text('body')
                
                # æå–æ•°æ®
                result = {
                    'registrar': None,
                    'registrant_org': None,
                    'creation_date': None,
                    'expiry_date': None,
                    'nameservers': [],
                    'raw_whois': None,
                    'registry': None,
                    'status': [],
                    'llm_parsed': False
                }
                
                # æ­¥éª¤1: æå–Raw WHOISé™„è¿‘çš„æ–‡æœ¬
                whois_context = self.extract_whois_context(
                    page_text, 
                    "Raw WHOIS",
                    before_tokens=100,
                    after_tokens=1900
                )
                
                # æ­¥éª¤2: å¦‚æœæ‰¾åˆ°WHOISä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨DeepSeekè§£æ
                if whois_context and self.deepseek_key:
                    print(f"      ğŸ¤– ä½¿ç”¨DeepSeek LLMè§£æWHOISæ•°æ®...")
                    llm_result = await self.parse_with_deepseek(whois_context, domain)
                    
                    if llm_result:
                        # åˆå¹¶LLMè§£æçš„ç»“æœ
                        for key, value in llm_result.items():
                            if value:  # åªä½¿ç”¨éç©ºå€¼
                                result[key] = value
                        result['llm_parsed'] = True
                        result['raw_whois'] = whois_context[:500]  # ä¿å­˜éƒ¨åˆ†åŸå§‹æ•°æ®
                
                # æ­¥éª¤3: å°è¯•ç›´æ¥æå–Raw WHOISæ•°æ®åŒºå—ï¼ˆfallbackï¼‰
                if not result['llm_parsed']:
                    try:
                        for selector in ['pre', 'code', '[class*="whois"]', '[class*="raw"]']:
                            try:
                                whois_text = await page.inner_text(selector, timeout=3000)
                                if whois_text and len(whois_text) > 100:
                                    result['raw_whois'] = whois_text
                                    
                                    # å¦‚æœæœ‰DeepSeekï¼Œå°è¯•è§£æ
                                    if self.deepseek_key:
                                        print(f"      ğŸ¤– ä½¿ç”¨DeepSeekè§£æRaw WHOISå—...")
                                        llm_result = await self.parse_with_deepseek(whois_text, domain)
                                        if llm_result:
                                            for key, value in llm_result.items():
                                                if value:
                                                    result[key] = value
                                            result['llm_parsed'] = True
                                    else:
                                        # æ²¡æœ‰LLMï¼Œä½¿ç”¨æ­£åˆ™è§£æ
                                        parsed = self._parse_raw_whois(whois_text)
                                        result.update(parsed)
                                    break
                            except:
                                continue
                    except:
                        pass
                
                # æ­¥éª¤4: ä»æ•´ä¸ªé¡µé¢æ–‡æœ¬æå–ä¿¡æ¯ï¼ˆæœ€åçš„fallbackï¼‰
                if not result['registrar']:
                    match = re.search(r'Registrar:\s*([^\n\r]+)', page_text, re.I)
                    if match:
                        result['registrar'] = match.group(1).strip()
                
                if not result['creation_date']:
                    match = re.search(r'(?:Created|Creation Date|Registered):\s*([^\n\r]+)', page_text, re.I)
                    if match:
                        result['creation_date'] = match.group(1).strip()
                
                if not result['expiry_date']:
                    match = re.search(r'(?:Expir[^:]*Date|Registry Expiry Date):\s*([^\n\r]+)', page_text, re.I)
                    if match:
                        result['expiry_date'] = match.group(1).strip()
                
                if not result['nameservers']:
                    ns_matches = re.findall(r'(?:Name Server|Nameserver):\s*([^\n\r]+)', page_text, re.I)
                    if ns_matches:
                        result['nameservers'] = [ns.strip() for ns in ns_matches[:10]]
                
                if not result['registry']:
                    match = re.search(r'Registry:\s*([^\n\r]+)', page_text, re.I)
                    if match:
                        result['registry'] = match.group(1).strip()
                
                await browser.close()
                
                # æ£€æŸ¥æ˜¯å¦è·å–åˆ°ä»»ä½•æœ‰ç”¨ä¿¡æ¯
                has_data = any([
                    result['registrar'],
                    result['creation_date'],
                    result['raw_whois'],
                    result['nameservers'],
                    result['registry']
                ])
                
                # æ˜¾ç¤ºè§£ææ–¹å¼
                if result['llm_parsed']:
                    print(f"      âœ“ LLMè§£ææˆåŠŸ")
                elif has_data:
                    print(f"      âœ“ è§„åˆ™è§£ææˆåŠŸ")
                
                return {'source': 'whois_scraper', 'success': has_data, 'data': result}
        
        except Exception as e:
            print(f"      âŒ Playwrightçˆ¬å–å¤±è´¥: {e}")
            return {'source': 'whois_scraper', 'success': False, 'data': {}}
    
    def _parse_raw_whois(self, raw_text: str) -> Dict:
        """ä»raw WHOISæ–‡æœ¬è§£æä¿¡æ¯"""
        result = {}
        
        # æå–æ³¨å†Œå•†
        match = re.search(r'Registrar:\s*(.+)', raw_text, re.I)
        if match:
            result['registrar'] = match.group(1).strip()
        
        # æå–åˆ›å»ºæ—¥æœŸ
        match = re.search(r'(?:Created|Creation Date):\s*(.+)', raw_text, re.I)
        if match:
            result['creation_date'] = match.group(1).strip()
        
        # æå–è¿‡æœŸæ—¥æœŸ
        match = re.search(r'(?:Expir[^:]*Date|Registry Expiry Date):\s*(.+)', raw_text, re.I)
        if match:
            result['expiry_date'] = match.group(1).strip()
        
        # æå–åç§°æœåŠ¡å™¨
        ns_matches = re.findall(r'(?:Name Server|nameserver):\s*(.+)', raw_text, re.I)
        if ns_matches:
            result['nameservers'] = [ns.strip() for ns in ns_matches]
        
        return result
    
    def _parse_rdap_response(self, rdap_data: Dict, source_url: str) -> Dict:
        """è§£æRDAPå“åº”"""
        result = {
            'registrar': None,
            'registry': None,
            'creation_date': None,
            'expiry_date': None,
            'nameservers': [],
            'registrant_org': None,
            'data_source': source_url
        }
        
        # æå–æ³¨å†Œå•†
        entities = rdap_data.get('entities', [])
        for entity in entities:
            roles = entity.get('roles', [])
            if 'registrar' in roles:
                result['registrar'] = entity.get('vcardArray', [[]])[1][0][3] if entity.get('vcardArray') else None
                break
        
        # æå–æ—¥æœŸ
        events = rdap_data.get('events', [])
        for event in events:
            action = event.get('eventAction', '')
            date = event.get('eventDate', '')
            if action == 'registration':
                result['creation_date'] = date
            elif action == 'expiration':
                result['expiry_date'] = date
        
        # æå–åç§°æœåŠ¡å™¨
        nameservers = rdap_data.get('nameservers', [])
        result['nameservers'] = [ns.get('ldhName', '') for ns in nameservers]
        
        return result
    
    def _parse_api_ninjas_response(self, data: Dict) -> Dict:
        """è§£æAPI Ninjaså“åº”"""
        return {
            'registrar': data.get('registrar'),
            'creation_date': data.get('creation_date'),
            'expiry_date': data.get('expiration_date'),
            'nameservers': data.get('name_servers', []),
            'registrant_org': data.get('registrant_organization')
        }
    
    def _count_fields(self, data: Dict) -> int:
        """è®¡ç®—æœ‰æ•ˆå­—æ®µæ•°é‡"""
        count = 0
        for key, value in data.items():
            if key in ['source', 'success', 'data_source']:
                continue
            if value:
                if isinstance(value, list):
                    if len(value) > 0:
                        count += 1
                elif isinstance(value, str):
                    if value.strip():
                        count += 1
                elif value is not None:
                    count += 1
        return count
    
    def _select_best_result(self, results: List[Dict]) -> Dict:
        """é€‰æ‹©ä¿¡æ¯æœ€å¤šçš„ç»“æœ"""
        best = None
        best_count = 0
        
        for result in results:
            if not result['success']:
                continue
            
            count = self._count_fields(result['data'])
            
            if count > best_count:
                best_count = count
                best = result
        
        return best if best else results[0]
    
    async def query_dns_info(self, domain: str) -> Dict:
        """æŸ¥è¯¢DNSä¿¡æ¯"""
        info = {
            'a_records': [],
            'mx_records': [],
            'txt_records': [],
            'ns_records': []
        }
        
        resolver = dns.resolver.Resolver()
        resolver.timeout = 5
        resolver.lifetime = 5
        
        # Aè®°å½•
        try:
            answers = resolver.resolve(domain, 'A')
            info['a_records'] = [str(rdata) for rdata in answers]
        except:
            pass
        
        # MXè®°å½•
        try:
            answers = resolver.resolve(domain, 'MX')
            info['mx_records'] = [f"{rdata.preference} {rdata.exchange}" for rdata in answers]
        except:
            pass
        
        # TXTè®°å½•
        try:
            answers = resolver.resolve(domain, 'TXT')
            for rdata in answers:
                txt_value = ''.join([s.decode('utf-8') if isinstance(s, bytes) else str(s) for s in rdata.strings])
                info['txt_records'].append(txt_value)
        except:
            pass
        
        # NSè®°å½•
        try:
            answers = resolver.resolve(domain, 'NS')
            info['ns_records'] = [str(rdata) for rdata in answers]
        except:
            pass
        
        return info
    
    async def process_domain(self, domain: str, iteration: int = 0) -> Dict:
        """å¤„ç†å•ä¸ªåŸŸå - å®˜æ–¹WHOISä¼˜å…ˆ + å¤šå±‚fallback"""
        print(f"\n   [{iteration}] å¤„ç†: {domain}")
        
        # ç­–ç•¥1: ä¼˜å…ˆä½¿ç”¨å®˜æ–¹WHOISæœåŠ¡å™¨ï¼ˆç›´æ¥socketè¿æ¥ï¼‰
        whois_result = await self.query_official_whois(domain)
        
        if whois_result['success']:
            best_result = whois_result
            # æ˜¾ç¤ºç»“æœ
            count = self._count_fields(whois_result.get('data', {}))
            llm_used = whois_result.get('data', {}).get('llm_parsed', False)
            method = "LLMè§£æ" if llm_used else "è§„åˆ™è§£æ"
            print(f"      âœ“ å®˜æ–¹WHOISæˆåŠŸ ({method}): {count}ä¸ªå­—æ®µ")
        else:
            # ç­–ç•¥2: å®˜æ–¹WHOISå¤±è´¥ï¼Œå°è¯•RDAP API
            print(f"      ğŸ“¡ å®˜æ–¹WHOISå¤±è´¥ï¼Œå°è¯•RDAP API...")
            api_tasks = [
                self.query_rdap_primary(domain),
                self.query_rdap_bootstrap(domain),
                self.query_api_ninjas(domain)
            ]
            
            api_results = await asyncio.gather(*api_tasks, return_exceptions=True)
            valid_results = [r for r in api_results if isinstance(r, dict)]
            
            # æ˜¾ç¤ºå„APIç»“æœ
            for r in valid_results:
                status = "âœ“" if r['success'] else "âœ—"
                count = self._count_fields(r.get('data', {})) if r['success'] else 0
                print(f"      {status} {r['source']}: {count}ä¸ªå­—æ®µ")
            
            # é€‰æ‹©æœ€ä½³ç»“æœ
            best_result = self._select_best_result(valid_results)
            
            # ç­–ç•¥3: æ‰€æœ‰APIéƒ½å¤±è´¥ï¼Œä½¿ç”¨Playwrightçˆ¬è™«
            if not best_result['success']:
                print(f"      âš ï¸  æ‰€æœ‰APIå¤±è´¥ï¼Œå¯åŠ¨Playwrightçˆ¬è™«...")
                scrape_result = await self.scrape_whois_website(domain)
                if scrape_result['success']:
                    best_result = scrape_result
                    count = self._count_fields(scrape_result['data'])
                    print(f"      âœ“ çˆ¬è™«æˆåŠŸ: {count}ä¸ªå­—æ®µ")
                else:
                    print(f"      âœ— çˆ¬è™«ä¹Ÿå¤±è´¥")
        
        # 4. æŸ¥è¯¢DNSä¿¡æ¯
        dns_info = await self.query_dns_info(domain)
        
        # 5. åˆå¹¶ç»“æœ
        final_result = {
            'domain': domain,
            'data_source': best_result['source'],
            'rdap_data': best_result.get('data', {}),
            'dns_info': dns_info,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'success': best_result['success']
        }
        
        # æ˜¾ç¤ºæ‘˜è¦
        rdap_count = self._count_fields(best_result.get('data', {}))
        dns_count = sum(1 for v in dns_info.values() if v)
        print(f"      âœ… å®Œæˆ: WHOIS/RDAP={rdap_count}å­—æ®µ, DNS={dns_count}ç±»å‹")
        
        return final_result
    
    async def monitor_domains(self, domains: List[str], interval: int = 300, max_iterations: int = None):
        """æŒç»­ç›‘æ§åŸŸååˆ—è¡¨"""
        print("\n" + "="*70)
        print("ğŸ”„ å¢å¼ºåŸŸåç›‘æ§ç³»ç»Ÿ - æŒç»­ç›‘æ§æ¨¡å¼")
        print("="*70)
        print(f"\nğŸ“Š é…ç½®:")
        print(f"   åŸŸåæ•°é‡: {len(domains)}")
        print(f"   ç›‘æ§é—´éš”: {interval}ç§’ ({interval//60}åˆ†é’Ÿ)")
        print(f"   æœ€å¤§è¿­ä»£: {max_iterations if max_iterations else 'æ— é™åˆ¶'}")
        print(f"   æ•°æ®æº: å®˜æ–¹WHOIS â†’ RDAP API â†’ Playwrightçˆ¬è™«")
        print(f"   LLMè§£æ: {'å¯ç”¨ (DeepSeek)' if self.deepseek_key else 'ç¦ç”¨'}")
        print("\n" + "="*70)
        
        iteration = 0
        
        while True:
            iteration += 1
            
            if max_iterations and iteration > max_iterations:
                print(f"\nâœ“ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œåœæ­¢ç›‘æ§")
                break
            
            print(f"\n\n{'='*70}")
            print(f"ğŸ“ ç¬¬ {iteration} è½®ç›‘æ§ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"{'='*70}")
            
            start_time = datetime.now()
            
            # å¤„ç†æ‰€æœ‰åŸŸå
            results = []
            for i, domain in enumerate(domains, 1):
                result = await self.process_domain(domain, i)
                results.append(result)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                if i < len(domains):
                    await asyncio.sleep(1)
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            
            # JSON
            json_file = f"monitor_results_iter{iteration}_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'iteration': iteration,
                    'timestamp': datetime.now(timezone.utc).isoformat(),
                    'domains_count': len(domains),
                    'results': results
                }, f, indent=2, ensure_ascii=False, default=str)
            
            # CSV
            csv_file = f"monitor_results_iter{iteration}_{timestamp}.csv"
            self._save_csv(results, csv_file)
            
            # ç»Ÿè®¡
            success_count = sum(1 for r in results if r['success'])
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            print(f"\n{'='*70}")
            print(f"ğŸ“Š ç¬¬ {iteration} è½®ç»Ÿè®¡:")
            print(f"   âœ… æˆåŠŸ: {success_count}/{len(domains)} ({success_count*100//len(domains)}%)")
            print(f"   â±ï¸  è€—æ—¶: {elapsed:.1f}ç§’")
            print(f"   ğŸ’¾ ä¿å­˜: {json_file}")
            print(f"   ğŸ’¾ ä¿å­˜: {csv_file}")
            
            # ç­‰å¾…ä¸‹ä¸€è½®
            if max_iterations is None or iteration < max_iterations:
                print(f"\nâ³ ç­‰å¾… {interval} ç§’åå¼€å§‹ä¸‹ä¸€è½®ç›‘æ§...")
                print(f"   (æŒ‰ Ctrl+C å¯éšæ—¶åœæ­¢)")
                
                try:
                    await asyncio.sleep(interval)
                except KeyboardInterrupt:
                    print(f"\n\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢ç›‘æ§")
                    break
    
    def _save_csv(self, results: List[Dict], filename: str):
        """ä¿å­˜CSVæ–‡ä»¶"""
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # è¡¨å¤´
            writer.writerow([
                'domain', 'success', 'data_source', 'registrar', 'registry',
                'creation_date', 'expiry_date', 'rdap_nameservers',
                'dns_a_records', 'dns_mx_records', 'dns_txt_records',
                'dns_ns_records', 'timestamp'
            ])
            
            # æ•°æ®
            for r in results:
                rdap = r.get('rdap_data', {})
                dns = r.get('dns_info', {})
                
                writer.writerow([
                    r['domain'],
                    'Y' if r['success'] else 'N',
                    r['data_source'],
                    rdap.get('registrar', ''),
                    rdap.get('registry', ''),
                    rdap.get('creation_date', ''),
                    rdap.get('expiry_date', ''),
                    '; '.join(rdap.get('nameservers', [])),
                    '; '.join(dns.get('a_records', [])),
                    '; '.join(dns.get('mx_records', [])),
                    '; '.join(dns.get('txt_records', [])),
                    '; '.join(dns.get('ns_records', [])),
                    r['timestamp']
                ])


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¢å¼ºåŸŸåç›‘æ§ç³»ç»Ÿ (æ”¯æŒLLMè§£æ)')
    parser.add_argument('csv_file', help='è¾“å…¥CSVæ–‡ä»¶')
    parser.add_argument('--interval', type=int, default=300, help='ç›‘æ§é—´éš”(ç§’)ï¼Œé»˜è®¤300ç§’(5åˆ†é’Ÿ)')
    parser.add_argument('--iterations', type=int, default=None, help='æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤æ— é™')
    parser.add_argument('--api-key', help='API Ninjaså¯†é’¥')
    parser.add_argument('--deepseek-key', help='DeepSeek APIå¯†é’¥(ç”¨äºLLMè§£æWHOIS)')
    
    args = parser.parse_args()
    
    # è¯»å–åŸŸå
    if not Path(args.csv_file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.csv_file}")
        return
    
    domains = []
    with open(args.csv_file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        for row in reader:
            if len(row) >= 2 and row[1].strip():
                domains.append(row[1].strip())
    
    if not domains:
        print("âŒ CSVä¸­æ²¡æœ‰åŸŸå")
        return
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = EnhancedDomainMonitor(
        api_ninjas_key=args.api_key,
        deepseek_key=args.deepseek_key
    )
    
    # æ˜¾ç¤ºLLMçŠ¶æ€
    if monitor.deepseek_key:
        print("ğŸ¤– DeepSeek LLMå·²å¯ç”¨ - å°†ç”¨äºè§£æWHOISæ•°æ®")
    else:
        print("â„¹ï¸  DeepSeek LLMæœªé…ç½® - å°†ä½¿ç”¨è§„åˆ™è§£æ")
    
    # å¼€å§‹ç›‘æ§
    try:
        await monitor.monitor_domains(
            domains=domains,
            interval=args.interval,
            max_iterations=args.iterations
        )
    except KeyboardInterrupt:
        print("\n\nâœ“ ç›‘æ§å·²åœæ­¢")


if __name__ == "__main__":
    asyncio.run(main())

